For training large neural networks you often need expensive hardware if you want to do it on your own. 
The way to do it these days is to use cloud services such as Google or Amazon to run the machine learning once you have prepared the dataset and the model. 
These services are often paid by the hour, but it's not expensive as long as you remember to shut it down after use, and only run it when you really need to.

Our dataset, however, only contains 55 participants and only a few days of activity measurements for each participant, 
so using a service to train the model would not be necessary. We decided to train them on one of our own personal computers, 
with an Nvidia GTX 1070. Our guess is that this GPU will do the job just fine. 
 
\section{Regression}

\begin{figure}
      \includegraphics[height=9cm]{img/regression/results_kerasregressor_1k_epochs.png}
      \caption{Regression Training Loss (MSE) by Epoch}
      \label{figure:regression_training_loss}
\end{figure}

\begin{figure}
      \includegraphics[height=8cm]{img/regression/confusion_kerasregressor_grouped.png}
      \caption{Confusion Matrices for Regression}
      \label{figure:regression_test_confusion}
\end{figure}

We ended up training the regression model \ref{code:regression_model} for 1000 epochs using a batch size of 16. Because of the simplicity of the model, 
1000 epochs did not take that much time and the training loss graph \ref{figure:regression_training_loss} shows that more epochs won't yield any better results. 
The loss does not reduce any more after the first 100 epochs for any of the columns. As we said earlier, we wanted the model to predict 
flawlessly based on the MADRS scores, and the loss results from training these are clearly better than for the other columns. 

We used the \textbf{train\_test\_split} function to create training and testing data, and because of how few rows there are in this dataset, 
we used an 80/20 split for training and testing data. 20\% of the dataset in the test data seemed to be good enough for this experiment 
because we wanted as many rows as possible to train on. Note that the data elements that were in the test and train splits were different for each column. 

\subsection{Results}

The confusion matrices \ref{figure:regression_test_confusion} displays the results when running the prediction on the test data (and then using it as classification). 
We said we wanted 100\% on all metrics for the MADRS score columns, and this is clearly achieved because the model guessed everything right. 
The other columns have these performance scores (using \textbf{condition} as positive and \textbf{control} as negative):

\begin{multicols}{2}
\subsection{Accuracy}
$ A = \frac{TP+TN}{TP+TN+FN+FP} $
\\\\
- \textbf{Gender}: $\frac{7+0}{7+0+4+0} = 7/11 \approx 63\%$\\\\
- \textbf{Age}: $\frac{6+1}{6+1+3+1} = 7/11 \approx 63\%$\\\\
- \textbf{Days}: $\frac{5+0}{5+0+6+0} = 5/11 \approx 45\%$\\

\subsection{Precision}
$ P = \frac{TP}{TP+FP} $
\\\\
- \textbf{Gender}:$ \frac{7}{7+0} = 7/7 = 100\%$\\\\
- \textbf{Age}: $\frac{6}{6+1} = 6/7 \approx 86\%$\\\\
- \textbf{Days}: $\frac{5}{5+0} = 5/5 = 100\%$\\

\subsection{Recall}
$ R = \frac{TP}{TP+FN} $
\\\\
- \textbf{Gender}: $\frac{7}{7+4} = 7/11 \approx 63\%$\\\\
- \textbf{Age}: $\frac{6}{6+3} = 6/9 \approx 67\%$\\\\
- \textbf{Days}: $\frac{5}{5+6} = 5/11 \approx 45\%$

\subsection{Specificity}
$ S = \frac{TN}{TN+FP} $
\\\\
- \textbf{Gender}: $\frac{0}{0+0} = 0/0$\\\\
- \textbf{Age}: $\frac{1}{1+1} = 1/2 = 50\%$\\\\
- \textbf{Days}: $\frac{0}{0+0} = 0/0$\\

\end{multicols}

\subsection{F1 Score}
$ F1 = 2 \cdot \frac{P \cdot R}{P + R} $
\\\\
- \textbf{Gender}: $2 \cdot \frac{1 \cdot 0,63}{1 + 0,63} \approx 77\%$\\\\
- \textbf{Age}: $2 \cdot \frac{0,86 \cdot 0,67}{0,86 + 0,67} \approx 75\%$\\\\
- \textbf{Days}: $2 \cdot \frac{1 \cdot 0,45}{1 + 0,45} \approx 62\%$\\

Looking away from the results of \textbf{madrs1} and \textbf{madrs2}, because they were obviously going to be good, these performance scores tell us that the model 
is good at predicting that someone is in the condition group (see precision). Other than that, we can't really learn that much from them. 
Also, we have undefined specificity for both gender and days because we don't have any predictions for the values required to calculate them. 

However, we learned a lot about machine learning and regression doing this experiment, and we look at it more like a warmup for the more complex models that we built 
for the next goals.

\section{1D CNN: Control vs Condition groups}

Next up was our goal to classify whether a participant belongs to the control group or the condition group. 
As we said in the section about optimizing the model, the segment length is what we thought was going to impact the result the most. 
To test this theory, we trained the model to fit input data created with segment lengths of 1, 2, 4, 8, 16, 24, 48 and 96 hours. 

The input data was split using \textbf{train\_test\_split} so that the training data was 80\% of the total and the rest was going to be used for 
testing after the training was complete. Of the training data, 40\% was set to be used as validation data, which is what the model uses in 
each epoch to determine if the results are getting better or worse. 

We did the training for each of the 8 different input sets for 10 epochs. To keep it simple, we used a batch size of 16 and 
the Adam optimizer with a default learning rate of 0,001 throughout this experiment. The main objective here was to find the best segment length to use, 
and not to train the models to be perfect, so these hyper-parameters seemed fine for this purpose. Our guess before we started with the experiment was 
that the more hours of data we used, the better, meaning that 96 hours of data in each segment was going to give us the best model 
(from what was possible with only 10 epochs). 

Looking at the history graphs \ref{figure:control_condition_10e} for how the training went epoch by epoch for each of the experiment's datasets, 
we noticed that the results were better when increasing the number of hours up to 48, and then it did not seem to be any better for 96 hours. 
This was the case for both training and testing, as the evaluation graphs (results after testing the model with the test-split) to the right also show straight lines from 
48 hours to 96 hours. The question was whether 48 hours was our magic number, or if we just needed to train it more.

To really find the best segment length, we experimented with more epochs. We trained again for 50 epochs, with 48, 72 and 96 hour long segments. 
From these results \ref{figure:control_condition_50e} it's clear that nothing more was achieved with segments longer than 48 hours. 

Moving forward using 48 hours segment length, the classifications you can see in the confusion matrices \ref{figure:control_condition_confusion_matrix_48h} 
are really close to perfect. The extra 40 epochs of training reduced the false positive classifications by a little bit, which was worth the time in our opinion as 
you want the value of wrong classifications to be as close to 0 as possible. For a relatively small dataset, we are very happy with a classification model 
with scores above 99\% on all performance metrics previously discussed. 

\begin{figure}
      \begin{multicols}{2}
            \includegraphics[height=5cm]{img/control_condition/plot_acc_train.png}
            \includegraphics[height=5cm]{img/control_condition/plot_loss_train.png}

            \includegraphics[height=5cm]{img/control_condition/plot_acc_eval.png}
            \includegraphics[height=5cm]{img/control_condition/plot_loss_eval.png}
      \end{multicols}
      \caption{Training (left) and testing (right) results (10 epochs)}
      \label{figure:control_condition_10e}
\end{figure}

\begin{figure}
      \begin{multicols}{2}
            \includegraphics[height=5cm]{img/control_condition/plot_acc_train_50e.png}
            \includegraphics[height=5cm]{img/control_condition/plot_loss_train_50e.png}

            \includegraphics[height=5cm]{img/control_condition/plot_acc_eval_50e.png}
            \includegraphics[height=5cm]{img/control_condition/plot_loss_eval_50e.png}
      \end{multicols}
      \caption{Training (left) and testing (right) results (50 epochs)}
      \label{figure:control_condition_50e}
\end{figure}

\begin{figure}
      \begin{multicols}{2}
            \includegraphics[height=4.5cm]{img/control_condition/conf_2880_60_10_32.png}
            \includegraphics[height=4.5cm]{img/control_condition/conf_2880_60_50_32.png}
      \end{multicols}
      \caption{10 epochs (left) and 50 epochs (right) for 48 hour segments}
      \label{figure:control_condition_confusion_matrix_48h}
\end{figure}

%TP   FN   
%FP   TN

% \newpage IF NEEDED

\subsection{Performance metrics}

\subsubsection{Accuracy}
$ A = \frac{TP+TN}{TP+TN+FN+FP} = \frac{3122+1577}{3122+1577+5+13} \approx 99,6\%$

\subsubsection{Precision}
$ P = \frac{TP}{TP+FP} = \frac{3122}{3122+13} \approx 99,6\%$

\subsubsection{Recall}
$ R = \frac{TP}{TP+FN} = \frac{3122}{3122+5} \approx 99,8\%$

\subsubsection{Specificity}
$ S = \frac{TN}{TN+FP} = \frac{1577}{1577+13} \approx 99,2\%$

\subsubsection{F1 Score}
$ F1 = 2 \cdot \frac{P \cdot R}{P + R} = 2 \cdot \frac{0,996 \cdot 0,998}{0,996 + 0,998} \approx 99,7\% $

\subsection{Cross-validation}
TODO

\section{1D CNN: Depression Classes}
\subsection{Performance metrics}
\subsubsection{Accuracy}
\subsubsection{Precision}
\subsubsection{Recall}
\subsubsection{Specificity}
\subsubsection{F1 Score}

\subsection{Cross-validation}

%\section{1D CNN: MADRS Score Prediction}
%\subsection{Performance metrics}
%\subsubsection{Accuracy}
%\subsubsection{Precision}
%\subsubsection{Recall}
%\subsubsection{Specificity}
%\subsubsection{F1 Score}
%\subsection{Cross validation}