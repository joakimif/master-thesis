%\[B\](.*?)\[/B\] => \\textbf{$1}

\section{Regression}

Regression is used a lot in statistics, where its main purpose is to predict a target value based on independent predictors and is useful to find cause and effect 
relationships between variables \cite{linear_regression_ml}. One experiment that might be interesting is to do linear regression on some of the columns in the demographics dataset 
\ref{figure:demographics}, in order to see whether there are columns that tell more about the participant's group (control or condition).  

Not all of the columns are relevant though, for example, \textbf{number} is unique for each participant so it does not make sense to use. 
The column \textbf{melanch} also does not make sense to use, as there is only one participant with Melancholia. 
\textbf{Inpatient} cannot be used because the participants in the control group are not patients, and can't be either inpatient or outpatient. 
The same goes for \textbf{edu}, \textbf{work} and \textbf{marriage}, but we don't have these data values for control group participants. 
We don't have their \textbf{afftype} or any of the \textbf{MADRS} scores either, but we already know them: \textbf{afftype} should be 0 (not bipolar) 
and both \textbf{MADRS} scores should be 0 (not depressed). 

We used the \textbf{afftype} column as a target in the regression. This way we could classify whether a participant is in the control or the condition group, 
by setting the \textbf{afftype} value to either 0 or 1 instead of 0, 1, 2 or 3 (values above 1 are reduced to 1), and run regression on all of the remaining 
columns one by one:

\begin{itemize}
      \item \textbf{Gender}
      \item \textbf{Age}
      \item \textbf{Days}
      \item \textbf{MADRS1}
      \item \textbf{MADRS2}
\end{itemize}

\input{code/tex/regression}

The model for this task is really simple. We created a sequential model \ref{code:regression_model} with one input layer and one output layer. 
The input is a \textbf{dense} layer which takes one value (the value of the current column), activates using \textbf{relu} and outputs 5 neurons. 
In the output layer, we simply activate using a \textbf{linear} function (default activation function when none are specified) and return one value: the prediction. 
The model is compiled with the loss function \textbf{Mean Squared Error} and the optimizer \textbf{Adam}. More on loss functions and optimizers will be described later. 

Since we were predicting the \textbf{afftype} of a participant, which is a binary value (0 or 1) and a prediction from the model yields a value between 0 and 1, 
we simply rounded the prediction value to the nearest integer after running predictions on the test data. Doing it this way instead of making a classification model, 
makes the \textbf{loss} value more useful as a metric while training rather than accuracy. 

When testing the model, we wanted to achieve an accuracy of 100\% when using the \textbf{MADRS} scores. This is because of how easy it is using a simple check: 
if the MADRS score is 0, then the participant is in the control group. If not, the participant is in the condition group. We didn't really need to use machine learning 
on these columns but was interesting to train the model to find this relationship without telling it the simple rule. For the other columns, we didn't really know what to expect; 
maybe there was a relationship, maybe not.

\section{1D Convolutional Neural Network}

\subsection{Convolution}
\begin{figure}
    \includegraphics[height=2.5cm]{img/feature_detector.png}
    \caption{Feature Detector / Filter "sliding" over input data}
    \label{figure:feature_detector}
\end{figure}

The main ingredient in a CNN is the \textit{convolutional} layer. It is responsible for, as you may have guessed, the convolution.
It consists of a number of \textit{filters} \ref{figure:feature_detector}, which are the sliding windows that go through the input data. 
They are also called \textit{feature detectors}, and using 100 of them means that the layer can detect 100 features. 
The size of a filter is called \textit{kernel size}. The output of convolutional layer is a matrix with one column for each filter, and one row for each step
in the convolution. How many steps there are, is given by the length of the input data (also called \textit{height}) minus the kernel size plus 1.


\subsection{Creating the Model}
Following the tutorial on 1D CNNs \cite{1d_cnn}, we came up with two models. 
One model for the first two goals was built for classification \ref{code:1d_conv_net_classifier}, and for the third goal we came up with another model built for 
prediction \ref{code:1d_conv_net_predictor}. We built the models a lot like how it was in the tutorial, 
but we also had to tweak some of the parameters to make the model work on our dataset.

To make a convolutional neural network, you need some \textit{convolutional} and \textit{pooling} layers.
Which layers you add in between and the ordering of them, together with the parameters you pass to the layers, 
is what makes the model perform differently. This, of course, requires your input data to be well structured.

\subsubsection{Classification}
The following model \ref{code:1d_conv_net_classifier} is used to achieve our first two goals; classifying whether a 
participant belongs to the \textbf{control} or \textbf{condition} group, and classifying the participant's \textbf{depression class}.
The only difference between these goals is the number of classes we are trying to classify, and therefore only the output layer needs to be changed.

\begin{enumerate}
  \item We start by defining a \textbf{Sequential} model. This is an easy to understand and readable way of defining a model. 
        Alternatively we could use a \textbf{functional} model, which gives more control of inputs and outputs of the layers.
        A \textbf{functional} model would be useful if we wanted to debug and optimize each layer within the model.
  \item \textbf{Reshape}: In the first layer we need to reshape the input data so that it becomes an $X$ by 1 matrix, where $X$ is the length of each segment.
        The reason for this reshape step is because the next layer (\textbf{Conv1D}) requires the input to contain (\textit{batch, steps, channels}). 
        \textit{Batch} will be set to \textbf{None}, \textit{steps} will be the segments, and \textit{channels} will be $1$ 
        (because we only have one measurement value for each minute).
  \item \textbf{Conv1D}: This is the first \textit{convolutional} layer, where the required parameters are how many \textit{filters} 
        you want, and how big the \textit{kernel} should be. As in the tutorial, we will also start by using 100 filters and a kernel size of 10. 
        Having less or more might impact the performance, which we will experiment on later.
        There are many different parameters that you can use on a layer like this, for example \textit{padding} and \textit{strides},
        but using the default values is good for now. This is also something that we can experiment with later.
        However using the default parameters, the output of this layer will be a $(X-10+1) \times 100$ matrix, 
        and $X$ is the length of each segment here as well. All the convolutional layers in this model will be using the activation function \textit{relu}.
  \item \textbf{Conv1D}: The second convolutional layer will look exactly like the first one, and the output will be a $(X-10+1-10+1) \times 100$ matrix. 
  \item \textbf{MaxPooling1D}: Pooling is important in a convolutional neural network to reduce complexity \cite{1d_cnn}. 
        The basic idea of \textit{max pooling} is to reduce to only the maximum value for each \textit{window} of size $N \times N$. We are using 2 as 
        window size ($N$), resulting in matrix that is half the size of the input: $ \frac{X-10+1-10+1}{2} \times 100$. 
        Pooling may also help reduce \textit{overfitting}, which is when the model learns its own training data too well, and performs worse on unseen data.
  \item \textbf{Conv1D}: Two more convolutional layers are added, and after these the input to the next layer will be a matrix of size
        $ \left( \frac{X-10+1-10+1}{2}-10+1-10+1 \right) \times 160 $.
  \item \textbf{GlobalAveragePooling1D}: This is another pooling layer, and this one takes the average of weights within the network instead of the maximum.
        After doing this, the matrix will be of size $ 1 \times 160 $.
  \item \textbf{Dropout}: Dropout is used to reduce overfitting, by randomly ignoring units in the neural network. 
  \item \textbf{Dense}: The final layer in the model is a dense layer (fully connected) which reduces the matrix from $160$ values to 
        either $2$ or $3$ (for goal one and two), with the activation function \textbf{softmax}. 
        Then the output (a $1$ in one of the neurons) is mapped to the corresponding label.
        
\end{enumerate}

After creating the model, we \textit{compile} it as a model that is ready to be \textit{fit} to the dataset, 
giving it a \textbf{loss function} and an \textbf{optimizer}.The loss function is the function that evaluates how well the 
model \textit{models} the given data \cite{loss_functions}, and the optimizer is the function that attempts to lower the output of the loss function. 

For the first two goals, the loss function \textit{categorical crossentropy} calculates a probability over the 
number of classes that are supplied (number of classes equals the number of neurons in the output layer). 

For the optimizer, there are many different choices available,
but to keep things simple, we will start out using an optimizer called \textbf{Adam} for training all of our models. Using a different optimizer can make the 
model fit to the dataset faster or slower. 

\subsubsection{Prediction}
To make the model work for our third goal, where we will predict the actual value for the participant's MADRS score, we have to change a few layers.
To simplify it a bit, we removed two of the \textit{Conv1D} layers, and after the \textit{GlobalAveragePooling1D} layer, this is applied:

\begin{enumerate}
      \setcounter{enumi}{7}
      \item \textbf{Flatten}: The matrix needs to be flat (one dimensional) before proceeding to the final layers.
      \item \textbf{Dense}: A dense layer with $10$ neurons, and \textit{relu} as the activation function. 
      \item \textbf{Dense}: The output layer is a dense layer of size $1$, since we are predicting \textit{one} value. 
            Also, the activation on this layer is going to be \textit{linear} instead of softmax.
\end{enumerate}

We compile this model with the loss function \textit{Mean Squared Error}, which is measured as the average (mean) 
of squared difference between predictions and actual observations ($ loss = \frac{\sum_{i=1}^{n}(y_i-\hat{y}_i)^2}{n} $) \cite{loss_functions}. 
The optimizer is the same as before.

\subsection{Creating Input and Output Data}
\begin{table}
  \begin{center}
    \begin{tabular}{| l | l |}
      \hline
      \textbf{Not bipolar} & \textbf{Bipolar}  \\ \hline
      0                    &  1                \\ \hline
      1                    &  0                \\ \hline
      0                    &  1                \\ \hline
      1                    &  0                \\ \hline
      1                    &  0                \\ \hline
      0                    &  1                \\ \hline
      0                    &  1                \\ \hline
      1                    &  0                \\ \hline
    \end{tabular}
    \caption{Categorical Labels}
    \label{table:categorical_labels}
  \end{center}
\end{table}

One function (\textit{create\_segments\_and\_labels()} \ref{code:reading_dataset}) is responsible of creating the data that is sent into the neural network. 
We start by defining a \textit{segment length} $L$, which is how much data (minutes) we want inside each segment. We will experiment with the value of $L$, 
but let's say we use segments of 4 hours at a time ($L=4*60=240$). Next we need a value for how many values to step after each iteration, $S$. 
We will keep this value at one hour, meaning $S=60$. Between the different goals, this function will only be different in how it determines the output values.
The code in the Source Code section \ref{code:reading_dataset} is simplified to only generate input and output for 
classifying control / condition groups (goal one).

\begin{itemize}
  \item First we read the \textit{global} dataset, where we find each participant and whether they are bipolar or not. As there is no \textit{afftype} value
        for non-bipolar participants, we simply set this to 0. This is fine because the other possible values are 1, 2 and 3.
  \item Then we iterate over all participant activity data files:
  \begin{itemize}
    \item Append a \textbf{segment} that is of length $L$ to the list of segments (using default parameters in the
          \textit{create\_segments\_and\_labels} function \ref{code:reading_dataset}).
    \item Append the target value for the current goal, so:
      \begin{itemize}
            \item Append a $1$ or $0$ for classifying control/condition group.
            \item Append a $0$, $1$ or $2$ for classifying depression class \\(normal/mild/moderate).
            \item Append the MADRS score (after measurement period) when the goal is to predict MADRS score.
      \end{itemize}  
    \item Skip $S$ indexes, and repeat until we have added all segments.
  \end{itemize}
  \item Make the list of labels into a \textit{categorical} 2D matrix \ref{table:categorical_labels} with a \textbf{1} in only one of the columns,
        instead of a single-dimensional list.
        This is only needed in the first two goals, for the \textbf{softmax} activation function.
  \item Also we need the list of segments to be restructured. We do this with the \textbf{reshape} function, 
        and after this the data is ready to be passed into the neural network.
\end{itemize}

\subsubsection{Train and test data}
One last step is done before we can start training the model. We need to split the data in two parts; training and test data. This way we can calculate 
the performance of the model after training has finished, and also prevent overfitting by evaluating on data that is unseen to the model. 

The function \textit{train\_test\_split} from the \textit{sklearn} package is useful here, where you input the segments and labels, 
plus how large you want the training and test sets to be (number between 0 and 1, which determines the size of the test partition). 
The function also randomizes the data, preventing model to accidentally learn something that is correct for segments in a row that 
also are chronologically in order. After calling the function \ref{code:sklearn_train_test_split}, you end up with two arrays for input data
(\textit{X\_train, X\_test}), and two arrays for output data (\textit{y\_train, y\_test}). In this case the \textit{test\_size} is set to $0.4$, 
meaning that the test sets contains $40\%$ of the total dataset and the remaining $60\%$ are in the training sets.

\begin{code}
    \caption{Sklearn train and test split}
    \label{code:sklearn_train_test_split}
    
    \begin{minted}[linenos]{python}
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(segments, 
                                                    labels, 
                                                    test_size=0.4)
    \end{minted}
\end{code}

\noindent Another option is to use something called \textit{K-fold cross-validation}. It works by splitting the dataset into $K$ train and test sets, 
and for each of them, train the model on the training set and validate on the test set. This is a good way of checking if your model's performance 
truly is independent on which data it trained on. The higher number of splits ($K$) means less data to train on, so you need to keep that in mind 
(also applies to \textit{train\_test\_split} if you set the \textit{test\_size} too high). Sklearn has an implementation of \textit{K-fold} 
splitting of data, called \textit{StratifiedKFold}. For example, code \ref{code:sklearn_k_fold} shows how you can use \textit{StratifiedKFold}
to perform a 3-fold cross-validation in your project ($K = 3$). 

\begin{code}
    \caption{Sklearn K-Fold}
    \label{code:sklearn_k_fold}
    
    \begin{minted}[linenos]{python}
    from sklearn.model_selection import StratifiedKFold

    segments, labels = create_segments_and_labels(...)

    skf = StratifiedKFold(n_splits=3, shuffle=True)

    for train_index, test_index in skf.split(segments, labels):
        X_train, X_test = segments[train_index], segments[test_index]
        y_train, y_test = labels[train_index], labels[test_index]

        model = create_model(...)
        model.fit(X_train, y_train, ...)
        results = model.evaluate(X_test, y_test)

        # do something with results...

    \end{minted}
\end{code}

\section{Optimizing the model}

\noindent Out of the box, we didn't think that the model was going to perform perfectly. Therefore we needed to experiment with each parameter that we passed to
the model, and find the best ones. We had some ideas of what to change:

\begin{itemize}
    \item Use different segment lengths
    \item Tweak hyper-parameters
\end{itemize}

\noindent The first idea, using different segment lengths, was the one we thought was going to impact the results the most. Having more data inside each segment 
will give the neural network more opportunities to learn features, and then be better at its job of either classifying or predicting something. 
It each epoch of training took more time though, since each layer had to process more data. Whether it was worth waiting the extra training time 
when increasing the segment length, and if it was one specific segment length that was superior to the others, is something to be discuss later in another section. 

Hyper-parameters are all the \textit{higher-order} parameters we compile/train the model with. They are different from the parameters learned by
training the model, and need to be fixed for one session of model training. Finding the perfect ones can be crucial for a well performing model. 
Hyper-parameters that we use in our models include:

\begin{itemize}
    \item \textbf{Optimizer and learning rate}\\
        The \textbf{optimizer} function (as said earlier) is important to lower the loss value when fitting the dataset to the model. 
        Different optimizers have different input parameters, and the \textit{learning rate} is one that all optimizers use. 
        Tweaking the learning rate can yield better results, but the default learning rate for the chosen optimizer is always a good starting point because
        it's what the author of the optimizer set as default.
    \item \textbf{Epochs}\\
        Defines how many iterations of training that are to be executed, and in most cases more epochs yield better results up to a certain point.
    \item \textbf{Batch size}\\
        This is how much data that is processed at the same time each epoch, and the best batch size to use can be completely different on 
        two different models. 
    \item \textbf{Train/test split size}\\
        When building a neural network, we want as much data as possible to both train and to test on afterwards. We can't use all the data 
        in both cases, so the most balanced split size needs to be determined.
\end{itemize}

\newpage

\section{Source Code}
\subsection{Create Input Data}
\input{code/tex/create_segments_and_labels}

\newpage
\subsection{The Models}
\input{code/tex/1d_conv_net}

\section{Training the models}

For training large neural networks you often need expensive hardware if you want to do it on your own. 
The way to do it these days is to use cloud services such as Google or Amazon to run the machine learning once you have prepared the dataset and the model. 
These services are often paid by the hour, but it's not expensive as long as you remember to shut it down after use, and only run it when you really need to.

Our dataset, however, only contains 55 participants and only a few days of activity measurements for each participant, 
so using a service to train the model would not be necessary. We decided to train them on one of our own personal computers, 
with an Nvidia GTX 1070. Our guess is that this GPU will do the job just fine. 
 
\subsection{Regression}

\begin{figure}
      \includegraphics[height=9cm]{img/regression/results_kerasregressor_1k_epochs.png}
      \caption{Regression Training Loss (MSE) by Epoch}
      \label{figure:regression_training_loss}
\end{figure}

\begin{figure}
      \includegraphics[height=8cm]{img/regression/confusion_kerasregressor_grouped.png}
      \caption{Confusion Matrices for Regression}
      \label{figure:regression_test_confusion}
\end{figure}

We ended up training the regression model \ref{code:regression_model} for 1000 epochs using a batch size of 16. Because of the simplicity of the model, 
1000 epochs did not take that much time and the training loss graph \ref{figure:regression_training_loss} shows that more epochs won't yield any better results. 
The loss does not reduce any more after the first 100 epochs for any of the columns. As we said earlier, we wanted the model to predict 
flawlessly based on the MADRS scores, and the loss results from training these are clearly better than for the other columns. 

We used the \textbf{train\_test\_split} function to create training and testing data, and because of how few rows there are in this dataset, 
we used an 80/20 split for training and testing data. 20\% of the dataset in the test data seemed to be good enough for this experiment 
because we wanted as many rows as possible to train on. Note that the data elements that were in the test and train splits were different for each column. 

\subsubsection{Results}

The confusion matrices \ref{figure:regression_test_confusion} displays the results when running the prediction on the test data (and then using it as classification). 
We said we wanted 100\% on all metrics for the MADRS score columns, and this is clearly achieved because the model guessed everything right. 
The other columns have these performance scores (using \textbf{condition} as positive and \textbf{control} as negative):

\begin{multicols}{2}
\subsubsection{Accuracy}
$ A = \frac{TP+TN}{TP+TN+FN+FP} $
\\\\
- \textbf{Gender}: $\frac{7+0}{7+0+4+0} = 7/11 \approx 63\%$\\\\
- \textbf{Age}: $\frac{6+1}{6+1+3+1} = 7/11 \approx 63\%$\\\\
- \textbf{Days}: $\frac{5+0}{5+0+6+0} = 5/11 \approx 45\%$\\

\subsubsection{Precision}
$ P = \frac{TP}{TP+FP} $
\\\\
- \textbf{Gender}:$ \frac{7}{7+0} = 7/7 = 100\%$\\\\
- \textbf{Age}: $\frac{6}{6+1} = 6/7 \approx 86\%$\\\\
- \textbf{Days}: $\frac{5}{5+0} = 5/5 = 100\%$\\

\subsubsection{Recall}
$ R = \frac{TP}{TP+FN} $
\\\\
- \textbf{Gender}: $\frac{7}{7+4} = 7/11 \approx 63\%$\\\\
- \textbf{Age}: $\frac{6}{6+3} = 6/9 \approx 67\%$\\\\
- \textbf{Days}: $\frac{5}{5+6} = 5/11 \approx 45\%$

\subsubsection{Specificity}
$ S = \frac{TN}{TN+FP} $
\\\\
- \textbf{Gender}: $\frac{0}{0+0} = 0/0$\\\\
- \textbf{Age}: $\frac{1}{1+1} = 1/2 = 50\%$\\\\
- \textbf{Days}: $\frac{0}{0+0} = 0/0$\\

\end{multicols}

\subsubsection{F1 Score}
$ F1 = 2 \cdot \frac{P \cdot R}{P + R} $
\\\\
- \textbf{Gender}: $2 \cdot \frac{1 \cdot 0,63}{1 + 0,63} \approx 77\%$\\\\
- \textbf{Age}: $2 \cdot \frac{0,86 \cdot 0,67}{0,86 + 0,67} \approx 75\%$\\\\
- \textbf{Days}: $2 \cdot \frac{1 \cdot 0,45}{1 + 0,45} \approx 62\%$\\

Looking away from the results of \textbf{madrs1} and \textbf{madrs2}, because they were obviously going to be good, these performance scores tell us that the model 
is good at predicting that someone is in the condition group (see precision). Other than that, we can't really learn that much from them. 
Also, we have undefined specificity for both gender and days because we don't have any predictions for the values required to calculate them. 

However, we learned a lot about machine learning and regression doing this experiment, and we look at it more like a warmup for the more complex models that we built 
for the next goals.

\subsection{1D CNN: Control vs Condition Group}


\begin{figure}
      \includegraphics[height=8cm]{img/control_condition/plot_acc_train.png}
      \caption{Training accuracy (10 epochs) for different segment lengths}
      \label{figure:control_condition_acc_train}

      \includegraphics[height=8cm]{img/control_condition/plot_loss_train.png}
      \caption{Training loss (10 epochs) for different segment lengths}
      \label{figure:control_condition_loss_train}
\end{figure}

\begin{figure}
      \includegraphics[height=8cm]{img/control_condition/plot_acc_eval.png}
      \caption{Evaluation accuracy (10 epochs) for different segment lengths}
      \label{figure:control_condition_acc_eval}

      \includegraphics[height=8cm]{img/control_condition/plot_loss_eval.png}
      \caption{Evaluation loss (10 epochs) for different segment lengths}
      \label{figure:control_condition_loss_eval}
\end{figure}

\begin{figure}
      \includegraphics[height=8cm]{img/control_condition/plot_acc_train_50e.png}
      \caption{Training accuracy (50 epochs) for different segment lengths}
      \label{figure:control_condition_acc_train_50e}

      \includegraphics[height=8cm]{img/control_condition/plot_loss_train_50e.png}
      \caption{Training loss (50 epochs) for different segment lengths}
      \label{figure:control_condition_loss_train_50e}
\end{figure}

\begin{figure}
      \includegraphics[height=8cm]{img/control_condition/conf_240_60_10_32.png}
      \caption{Confusion Matrix: 4 hours (10 epochs)}
      \label{figure:control_condition_confusion_matrix_4h}

      \includegraphics[height=8cm]{img/control_condition/conf_2880_60_10_32.png}
      \caption{Confusion Matrix: 48 hours (10 epochs)}
      \label{figure:control_condition_confusion_matrix_48h}
\end{figure}

\begin{figure}
      \includegraphics[height=8cm]{img/control_condition/conf_2880_60_50_32.png}
      \caption{Confusion Matrix: 48 hours (50 epochs)}
      \label{figure:control_condition_confusion_matrix_48h_50e}
\end{figure}

\subsection{1D CNN: Depression Classes}




\subsection{1D CNN: MADRS Score Prediction}