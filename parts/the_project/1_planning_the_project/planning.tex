\section{Goals}

Our goal in this thesis is to create machine learning models for three different tasks:

\begin{itemize}
  \item Predict whether a participant belongs to the \textbf{control} group or \textbf{condition} group.
  \item Predict a participant's depression class (by MADRS score).
  \item Predict a participant's MADRS score.
\end{itemize}

The model is going to be a One-Dimensional Convolutional Neural Network. 
We will solve all these using each participant's activity data as input, and we will create a model that, with few changes, can be used for all three goals.
For the different goals, only the last few layers (preferably only the output layer) should be changed. For the first goal, 
classifying \textbf{control} or \textbf{condition} group, the output data should be a matrix with two columns (\textbf{control}, \textbf{condition}), 
with a $1$ in one of the columns and a $0$ in the other for each row.

But first we wanted to see if this problem could be solved with simple regression. The idea was to simply throw in the columns from the 
demographics dataset \ref{figure:demographics}. We did not expect much from this, as there are only 55 rows in the table. Anyone having a little bit experience 
with machine learning will know that this is not nearly enough data. But we wanted to do it regardless, and see how a simple and stupid model performed.
Doing this, we would also establish some sort of benchmark for performance; the Convolutional Neural Network model must do \textit{at least} better than this one.

\subsection{Regression}

We decided to use a regression classifier for the first idea. However the dataset is not structured in a way such that we can just throw it into Keras.
First and foremost, most values in this table are blank for participants in the \textit{control group}, so it does only make sense to predict within 
the \textit{condition group}. 

The columns \textbf{number} and \textbf{days} should be dropped, as they probably have nothing to do with the result. 
The column \textbf{afftype} is be the column the regression model should guess, so it needed to be in a separate table (X = input table, y = output table).

Both the \textit{age} and \textit{education} columns are strings with values that the actual value is within. For example the participant \textbf{condition\_1} 
has age = \textbf{35-39} and edu = \textbf{6-10}. It would be better to change this to one value, and we decided to use the median of the two values.
Also, we changed some other values to make more sense. \textit{Melanch, inpatient, marriage, work and gender} are \textbf{binary} values, 
so we changed them to be either 0 or 1. We also changed \textit{afftype} (ternary value) to between 0 and 2 instead of between 1 and 3.

\newpage
After the changes, the tables should be read the following way:

X:
\begin{itemize}
  \item \textbf{gender}: 0 = male, 1 = female
  \item \textbf{age}: Median value of age range
  \item \textbf{melanch}: 0 = does not have melancholia, 1 = has melancholia
  \item \textbf{inpatient}: 0 = outpatient, 1 = inpatient
  \item \textbf{edu}: Median value of education range
  \item \textbf{marriage}: 0 = single, 1 = married (or cohabiting)
  \item \textbf{work}: 0 = not working, 1 = working
  \item \textbf{madrs1}: MADRS score before activity measurements
  \item \textbf{madrs2}: MADRS score after activity measurements
\end{itemize}

y:
\begin{itemize}
  \item \textbf{afftype}: 0 = bipolar II, 1 = unipolar depressive, 2 = bipolar I
\end{itemize}

\section{Convolutional Neural Network}

As we said before, we wanted to use a Convolutional Neural Network to achieve our goals. 

\subsection{Learning experiments}

We needed to learn more about CNNs. CNNs are used in image recognition, so we proceeded to implement one. We found a tutorial on how to make 
a 2D CNN for classifying cats and dogs from images \cite{2d_cnn}, and thought it would be a good way to learn.

It was both a fun and informative experience implementing this. Especially when we extended the script to allow an image url to predict on. 
Then we could browse for any image of a cat or a dog, and find out if the model could handle it (in most cases it did!). 
We even tried inputting images of humans to the model for fun. This experiment resulted in a lot of motivation for our task.

However as mentioned before, our data is one-dimensional, so a two-dimensional CNN would not be useful.

\begin{quote}
  \textit{A 1D CNN is very effective when you expect to derive interesting features from shorter (fixed-length) segments of the overall data set 
  and where the location of the feature within the segment is not of high relevance. This applies well to the analysis of time sequences of sensor data 
  (such as gyroscope or accelerometer data).} \cite{1d_cnn}
\end{quote}

To learn more about 1D CNNs, we followed a tutorial \cite{1d_cnn}, which used a dataset containing 
time-sliced accelerometer data from a smartphone on the participants waists. The goal for this CNN is to predict what a given person is doing 
at the time, given the accelerometer data for that time slice. What the given person is doing is one of the following:
\begin{itemize}
  \item Standing
  \item Walking
  \item Jogging
  \item Sitting
  \item Upstairs
  \item Downstairs
\end{itemize}

As we followed the tutorial and implemented the model, we learned a lot about how 1D CNNs work and how we should structure our own data. 

However, we also learned where our dataset could provide more data. 
What if the dataset contained the current mental state of the bipolar patient? Then someone could make some automated system that always can tell a patient 
whether they are normal, manic or depressive. However data collection for this kind of task would be difficult because we can't always know what the
patient thinks, nor does the patient themselves. The "tutorial" dataset is different because it is easy to differentiate physical states of the body
like standing or walking.

\subsection{Creating the input data - activity measurements}

As we said before, we wanted the input data to be exactly the same for each goal, since we wanted to use a similar model on all three.
The tutorial \cite{1d_cnn} sliced up the measurements with overlap, and labelled the slices. We are also did this.

We created a list where for each participant in the demographics table \ref{figure:demographics}, measurements for $N$ hours were grouped. 
Another choice we learned from the tutorial was to overlap the sequences, so we made the next group of $N$ hours start \textit{$M$} hours after, 
and not \textit{$N$} hours after the group before, as one might think. When this list was complete with sequences from all participants, we 
had to \textbf{reshape} it so that it could fit into a neural network. We ended up with a feature list (which we called \textbf{segments}), 
where each element was a list of activity measurements for 4 hours: 

\textbf{segments[0] = [[0], [143], [0], [20], [166], [160], [306], [277], [439], ...]}

\subsection{Creating the output data}

A second list was created simultaneously, where the value here was different on each goal. 

\subsubsection{Classifying participant group}
For classifying control / condition group, this list was built to contain the values \textbf{0} or \textbf{1} for the labels
\textbf{[CONTROL]} and \textbf{[CONDITION]}, which was chosen according to the group the participants were in. Using a helper function from Keras, 
\textbf{to\_categorical}, we transformed this list of labels into the matrix we described. Transforming the values to a
categorical matrix is required for the neural network that we ended up building, to be able to select a \textit{category} for the result. 
This list, which we called \textbf{labels}, looked like this: 

\textbf{labels[0] = [0, 1]}

\noindent Meaning that the first segment is labeled as \textbf{[CONDITION]}.

\subsubsection{Predicting depression class}


\subsubsection{Predicting MADRS Score}

\section{Performance Metrics}
\subsection{Confusion Matrix}

\begin{table}
  \begin{center}
    \begin{tabular}{| l | l | l | l |}
      \hline
                                    & \textbf{Actual: Negative} & \textbf{Actual: Positive} \\ \hline
      \textbf{Predicted: Negative}  & True Negative (TN)        & False Negative (FN)       \\ \hline
      \textbf{Predicted: Positive}  & False Positive (FP)       & True Positive (TP)        \\
      \hline
    \end{tabular}
    \caption{Confusion Matrix}
    \label{table:confusion_matrix}
  \end{center}
\end{table}

\begin{table}
  \begin{center}
    \begin{tabular}{| l | l | l | l |}
      \hline
                                      & \textbf{Actual: Not Bipolar} & \textbf{Actual: Bipolar} \\ \hline
      \textbf{Predicted: Not Bipolar} & \textbf{95}                  & 5                        \\ \hline
      \textbf{Predicted: Bipolar}     & 7                            & \textbf{93}              \\
      \hline
    \end{tabular}
    \caption{Confusion Matrix Example: Bipolar vs Not Bipolar}
    \label{table:confusion_matrix_bipolar}
  \end{center}
\end{table}

Table \ref{table:confusion_matrix} shows a \textit{confusion matrix}. It is a very common and easy to understand metric for classification models in machine 
learning, and is the basis for other of common performance metrics, which we will describe later in this section. It can tell you how well your model is performing, 
by having correlation values for the different predicted classes. In our planned model for 
predicting bipolar vs not bipolar, let's say we have 200 samples in our test data, a \textit{confusion matrix} for a good model would look like 
table \ref{table:confusion_matrix_bipolar}, with highest numbers in \textbf{True Positive} and \textbf{True Negative} and as low numbers as possible in 
\textbf{False Positive} and \textbf{False Negative}. Having a high number in \textbf{True Positive} means that the model is able to predict that a 
participant is bipolar if he or she actually is bipolar, and having a high number in \textbf{True Negative} means that the model is able to predict 
that a participant is not bipolar if he or she actually isn't. The other cases, \textbf{False Positive} and \textbf{False Negative}, is where the model
made a wrong prediction, and therefore as close these numbers are to zero the better our model is.

\subsection{Accuracy}

\blockquote[\cite{ml_metrics}]{Accuracy is a good measure when the target variable classes in the data are nearly balanced.}

When calculating the \textit{accuracy}, we sum up the correct predictions and divide that with the total number of predictions 
($ \frac{TP + TN}{TP + TN + FN + FP} $).
For our example (\ref{table:confusion_matrix_bipolar}), the \textit{accuracy} would be 
$ \frac{93 + 95}{93 + 95 + 5 + 7} = 94\% $. 
It is a good metric to use for our example because the number of samples for each variable class is well balanced 
($ 93+5=98 $ samples where \textbf{bipolar} was the correct option, and $ 7+95=102 $ samples where \textbf{not bipolar} was correct).

Terrible use of the \textit{accuracy} metric would be when one of the classes strongly dominates the samples. 
For example, if a model predicts \textbf{cancer} vs \textbf{no cancer}, and the samples contain 5 people with cancer and 
the 95 remaining people do not. The model would be terrible at predicting cancer and have an \textit{accuracy} score of $ 95\% $ \cite{ml_metrics}.

\subsection{Precision}

This performance metric operates entirely on the predicted positives, and it tells us how many \textbf{true positives} there is among 
\textbf{predicted positives} ($ \frac{TP}{TP + FP} $) \cite{ml_metrics}. 

Precision is better to use on \textit{unbalanced} classes than accuracy. The \textit{cancer vs no cancer} example, assuming it 
predicts no-one to have \textbf{cancer}, would yield a precision score of $ \frac{5}{5+95} = 5\% $. And our \textit{bipolar vs not bipolar} example
would result in a precision score of $ \frac{93}{93+7} = 93\% $.

\subsection{Recall}

\textit{Recall} is another useful performance metric. It tells us the relationship between \textbf{true positives} and \textbf{actual positives},
for example how many predicted bipolar participants there were among total bipolar participants.

The calculation of recall is done by dividing \textbf{true positives} by \textbf{true positives + false negatives} ($ \frac{TP}{TP+FN} $), 
which translates into $ \frac{93}{93+5} \approx 95\% $ for table \ref{table:confusion_matrix_bipolar}.

Choosing a metric to use from \textit{precision} or \textit{recall} depends on your goal. Try to achieve close to $ 100\% $ \textit{recall} 
if you want to reduce \textbf{false negatives}, and likewise with \textit{precision} if you want to reduce \textbf{false positives} \cite{ml_metrics}.

\subsection{Specificity}

As \textit{recall} operates on \textbf{actual positives}, \textit{specificity} is the exact opposite metric. It tells us the relationship between
\textbf{true negatives} and \textbf{actual negatives}. So if your goal is to reduce \textbf{false positives}, specificity is a valid choice.

\textit{Specificity} is calculated by dividing \textbf{true negatives} by \textbf{true negatives + false positives} ($ \frac{TN}{TN+FP} $). 
For table \ref{table:confusion_matrix_bipolar}, the \textit{specificity} score equals $ \frac{95}{95+7} \approx 93\% $.

\subsection{F1 Score}

The metrics that we have described in this section are all useful when determining whether your classification model is good enough. 
But the relationship between \textit{recall} and \textit{precision} and knowing when to use which can be confusing, at least if the different classes
are somewhere between completely unbalanced and perfectly balanced (for example a 35\% split). 

Therefore another metric called \textit{F1 Score} was created, which gives us a balanced value combining \textit{recall (R)} and \textit{precision (P)}.
The basic idea is to return the \textit{mean} value of the two scores (F1 = $ \frac{P + R}{2} $), but that would not be balanced if one score is
much lower than the other. F1 score actually uses something called \textit{harmonic mean} instead of the standard \textit{arithmetic mean}, 
and is calculated as $ 2 \cdot \frac{P \cdot R}{P + R} $ \cite{ml_metrics}. 

Following this formula, the F1 score for confusion matrix \ref{table:confusion_matrix_bipolar} becomes:

\[
  F1 = 2 \cdot \frac{P \cdot R}{P + R} = 2 \cdot \frac{0,93 \cdot 0,95}{0,93 + 0,95} \approx 94\%
\]

\subsection{Classification Report}

A machine learning framework for Python, \textit{sklearn}, includes a package with functions to calculate most of these scores. 
Simply import and use them like this: 

\input{code/tex/sklearn_metrics}

\begin{table}
  \begin{tabular}{| l | l | l | l | l |}
    \hline
                  & precision & recall  & f1-score & support \\ \hline
    class 0       & 0.05      & 1.00    & 0.67    & 1        \\
    class 1       & 0.00      & 0.00    & 0.00    & 1        \\
    class 2       & 1.00      & 0.67    & 0.80    & 3        \\
                  &           &         &         &          \\ 
    micro avg     & 0.60      & 0.60    & 0.60    & 5        \\
    macro avg     & 0.50      & 0.56    & 0.49    & 5        \\
    weighted avg  & 0.60      & 0.60    & 0.60    & 5        \\
    \hline
  \end{tabular}
  \caption{Classification Report}
  \label{table:classification_report}
\end{table}

The classification report returns a matrix (see table \ref{table:classification_report}) with \textit{precision}, 
\textit{recall}, \textit{F1 score} and another column called \textit{support} (simply how many samples of data there are for this class). 
The rows describe each class used for prediction \cite{sklearn_classification_report}.

\section{Benchmarks}
 