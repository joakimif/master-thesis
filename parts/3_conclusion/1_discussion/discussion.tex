\section{Convolutional Neural Networks for Mental Health Classification/Prediction}

We have evaluated how convolutional neural networks performed on motor activity measurements from bipolar and unipolar depressed patients (condition group) together with non-depressed control participants. We achieved our goals of creating classification models for detecting whether a participant belongs to the control group or the condition group, the depression class of a participant (not depressed, mild depression or moderately depressed), and finally a prediction model for estimating the MADRS score of participants. 

\subsection{Limited number participants}
First and foremost, the dataset consists of only 55 participants, which is very limited. It was a good set of data for us in our thesis, but in order to use it in the real world, more participants is a requirement. Even when splitting the data into multiple parts to train and test on different data and avoid overfitting, it is difficult to know how the model would perform on an arbitrary person. Measurements from more people and different ethnicities and age groups around the world would significantly increase the viability of the dataset. We suggest collecting more data before applying this research anywhere in the real world.
 
We think that the number of participants is the main issue that made the classifier only decent when leaving one participant out. The difference between people is far too big for our convolutional network to handle with a dataset containing such few participants. With activity measurements from more people, maybe the network would pick up and learn more similarities between people within the same \textit{group}, and be able to classify with higher performance. 

\subsection{Input data and hyper-parameters}
Detecting control/condition group was our first experiment, and we found that for our dataset the optimal amount of data inside each segment was 2880 (48 hours in minutes). Following our calculations, the optimal segment length was 5760 (96 hours in minutes) for the next experiment where the goal was to detect depression classes. When we predicted MADRS scores, the optimal segment length was 2880 once again. 

We are aware of the fact that our calculations only apply to the specific segment lengths tested, and testing the performance using longer segments may have resulted with something else. However, we did not prioritize to do more of such experiments, as the training time would increase too much if we were to continue using the hardware that we used in our other experiments. 

It is also possible to tweak hyper-parameters more than we ended up doing. In the first two experiments, we did not touch anything else than the length of the input segments and the number of epochs. It was first in the prediction model that we started experimenting with different optimizers and the learning rate parameter. 

We suggest that future research experiment more with hyper-parameters. However, it is difficult to know whether tweaking hyper-parameters is a potential fix to the poor performance in the leave one participant out experiment. We assume it would not help that much because the training accuracy always ended up above 99\%. 

\subsection{Compared to earlier research}
Earlier research on the topic of classifying depression is different from this thesis, as most of them compare how different types of machine learning performs classification. In contrast, we focused on one specific type of supervised machine learning: convolutional neural networks, which we used to achieve different goals. Instead of creating baseline models and comparing our results to them, we compare our work to what Garcia-Ceja, E. et al. achieved on the same dataset (we will only focus on our first goal in this comparison, as it is the only experiment they also performed). 

Overall, we achieved an F1-score of 0.70 for classifying control/condition group, which is slightly better than the F1-scores from the research of Garcia-Ceja, E. et al. without oversampling. They achieved 0.66 for the deep neural network and 0.67 for the random forest \cite{GarciaCeja2018_classification_bipolar}. When using SMOTE as a technique for generating more data, they increased their random forest F1-score to 0.73. A suggestion for future experiments is to attempt using the same sampling strategies as Garcia-Ceja, E. et al. on the data passed into our convolutional network, and check if the performance gets any better. 

Our convolutional network performed a little bit better than the random forest and the deep neural network of Garcia-Ceja, E. et al., but it was not as good as we hoped considering the results of other experiments. The question of whether convolutional networks as a machine learning approach is a solid option to use in mental health remains unanswered, as our results were only promising and not anywhere close to perfect. Nonetheless, our opinion is that it was a step in the right direction, and we hope researchers continue to explore this type of machine learning.

Garcia-Ceja, E. et al. also suggested future research to explore classification based on the MADRS scale, which we implemented as our second goal. The results were similar to our first experiment; not so good performance overall but able to classify most non-depressed participants correctly. 

\subsection{Use cases in the real world}

The field of mental health is still at an early stage when it comes to machine learning. Several researchers have done research comparing different approaches and algorithms to see which of them work better than others for detecting mental diagnoses. The results of these research papers are promising, but further work is necessary as the goal is to trust the decisions of machine learning someday. 

\subsubsection{Personal activity data}
The performance was better when training with data from all patients (accuracy scores above 99\%), which tells us that the models were able to pick up features successfully. However, we learned from the \textit{leave one participant out} experiment that the difference between people's activity behavior is too significant for those in the condition group (F1-score was only 0.64). 

Because of this, personal activity datasets is possibly a better way of using convolutional neural networks. If we continuously save motor activity for each participant and train a model specific to each participant, then the network can learn all there is to know about one person and for example, be used to detect bipolar state changes. Combined with the research of Gr√ºnerbl, A. et al., where they managed to identify state changes with an accuracy of 76\% using phone call logs and microphone data \cite{grunerbl_smartphone_bipolar}, development of more accurate detection systems could be possible.


\section{Challenges and ethical concerns}
In most projects in the medical fields, there are going to be ethical concerns and challenges with privacy. What happens if someone unauthorized for the data gets access to it? What if the database gets hacked? With new regulations (GDPR), which means that users have the right to be forgotten or deleted. However, in this project, all data is anonymized (only referenced by an id), so there will be no persons mentioned. If the dataset were not to be anonymized, and the patient's names were in it, things could get problematic if it got into the wrong hands. 
