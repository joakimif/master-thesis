\section{Regression}



First and foremost, most values in this table are blank for participants in the \textit{control group}, so it does only make sense to predict within 
the \textit{condition group}. 

The columns \textbf{number} and \textbf{days} should be dropped, as they probably have nothing to do with the result. 
The column \textbf{afftype} is be the column the regression model should guess, so it needed to be in a separate table (X = input table, y = output table).

Both the \textit{age} and \textit{education} columns are strings with values that the actual value is within. For example the participant \textbf{condition\_1} 
has age = \textbf{35-39} and edu = \textbf{6-10}. It would be better to change this to one value, and we decided to use the median of the two values.
Also, we changed some other values to make more sense. \textit{Melanch, inpatient, marriage, work and gender} are \textbf{binary} values, 
so we changed them to be either 0 or 1. We also changed \textit{afftype} (ternary value) to between 0 and 2 instead of between 1 and 3.

After the changes, the table should be read the following way:

X:
\begin{itemize}
  \item \textbf{gender}: 0 = male, 1 = female
  \item \textbf{age}: Median value of age range
  \item \textbf{melanch}: 0 = does not have melancholia, 1 = has melancholia
  \item \textbf{inpatient}: 0 = outpatient, 1 = inpatient
  \item \textbf{edu}: Median value of education range
  \item \textbf{marriage}: 0 = single, 1 = married (or cohabiting)
  \item \textbf{work}: 0 = not working, 1 = working
  \item \textbf{madrs1}: MADRS score before activity measurements
  \item \textbf{madrs2}: MADRS score after activity measurements
\end{itemize}

y:
\begin{itemize}
  \item \textbf{afftype}: 0 = bipolar II, 1 = unipolar depressive, 2 = bipolar I
\end{itemize}

\section{1D Convolutional Neural Network}

\subsection{Convolution}
\begin{figure}
    \includegraphics[height=2.5cm]{img/feature_detector.png}
    \caption{Feature Detector / Filter "sliding" over input data}
    \label{figure:feature_detector}
\end{figure}

The main ingredient in a CNN is the \textit{convolutional} layer. It is responsible for, as you may have guessed, the convolution.
It consists of a number of \textit{filters} \ref{figure:feature_detector}, which are the sliding windows that go through the input data. 
They are also called \textit{feature detectors}, and using 100 of them means that the layer can detect 100 features. 
The size of a filter is called \textit{kernel size}. The output of convolutional layer is a matrix with one column for each filter, and one row for each step
in the convolution. How many steps there are, is given by the length of the input data (also called \textit{height}) minus the kernel size plus 1.


\subsection{Creating the Model}
Following the tutorial on 1D CNNs \cite{1d_cnn}, we came up with two models. 
One model for the first two goals was built for classification \ref{code:1d_conv_net_classifier}, and for the third goal we came up with another model built for 
prediction \ref{code:1d_conv_net_predictor}. We built the models a lot like how it was in the tutorial, 
but we also had to tweak some of the parameters to make the model work on our dataset.

To make a convolutional neural network, you need some \textit{convolutional} and \textit{pooling} layers.
Which layers you add in between and the ordering of them, together with the parameters you pass to the layers, 
is what makes the model perform differently. This, of course, requires your input data to be well structured.

\subsubsection{Classification}
The following model \ref{code:1d_conv_net_classifier} is used to achieve our first two goals; classifying whether a 
participant belongs to the \textbf{control} or \textbf{condition} group, and classifying the participant's \textbf{depression class}.
The only difference between these goals is the number of classes we are trying to classify, and therefore only the output layer needs to be changed.

\begin{enumerate}
  \item We start by defining a \textbf{Sequential} model. This is an easy to understand and readable way of defining a model. 
        Alternatively we could use a \textbf{functional} model, which gives more control of inputs and outputs of the layers.
        A \textbf{functional} model would be useful if we wanted to debug and optimize each layer within the model.
  \item \textbf{Reshape}: In the first layer we need to reshape the input data so that it becomes an $X$ by 1 matrix, where $X$ is the length of each segment.
        The reason for this reshape step is because the next layer (\textbf{Conv1D}) requires the input to contain (\textit{batch, steps, channels}). 
        \textit{Batch} will be set to \textbf{None}, \textit{steps} will be the segments, and \textit{channels} will be $1$ 
        (because we only have one measurement value for each minute).
  \item \textbf{Conv1D}: This is the first \textit{convolutional} layer, where the required parameters are how many \textit{filters} 
        you want, and how big the \textit{kernel} should be. As in the tutorial, we will also start by using 100 filters and a kernel size of 10. 
        Having less or more might impact the performance, which we will experiment on later.
        There are many different parameters that you can use on a layer like this, for example \textit{padding} and \textit{strides},
        but using the default values is good for now. This is also something that we can experiment with later.
        However using the default parameters, the output of this layer will be a $(X-10+1) \times 100$ matrix, 
        and $X$ is the length of each segment here as well. All the convolutional layers in this model will be using the activation function \textit{relu}.
  \item \textbf{Conv1D}: The second convolutional layer will look exactly like the first one, and the output will be a $(X-10+1-10+1) \times 100$ matrix. 
  \item \textbf{MaxPooling1D}: Pooling is important in a convolutional neural network to reduce complexity \cite{1d_cnn}. 
        The basic idea of \textit{max pooling} is to reduce to only the maximum value for each \textit{window} of size $N \times N$. We are using 2 as 
        window size ($N$), resulting in matrix that is half the size of the input: $ \frac{X-10+1-10+1}{2} \times 100$. 
        Pooling may also help reduce \textit{overfitting}, which is when the model learns its own training data too well, and performs worse on unseen data.
  \item \textbf{Conv1D}: Two more convolutional layers are added, and after these the input to the next layer will be a matrix of size
        $ \left( \frac{X-10+1-10+1}{2}-10+1-10+1 \right) \times 160 $.
  \item \textbf{GlobalAveragePooling1D}: This is another pooling layer, and this one takes the average of weights within the network instead of the maximum.
        After doing this, the matrix will be of size $ 1 \times 160 $.
  \item \textbf{Dropout}: Dropout is used to reduce overfitting, by randomly ignoring units in the neural network. 
  \item \textbf{Dense}: The final layer in the model is a dense layer (fully connected) which reduces the matrix from $160$ values to 
        either $2$ or $3$ (for goal one and two), with the activation function \textbf{softmax}. 
        Then the output (a $1$ in one of the neurons) is mapped to the corresponding label.
        
\end{enumerate}

After creating the model, we \textit{compile} it as a model that is ready to be \textit{fit} to the dataset, 
giving it a \textbf{loss function} and an \textbf{optimizer}.The loss function is the function that evaluates how well the 
model \textit{models} the given data \cite{loss_functions}, and the optimizer is the function that attempts to lower the output of the loss function. 

For the first two goals, the loss function \textit{categorical crossentropy} calculates a probability over the 
number of classes that are supplied (number of classes equals the number of neurons in the output layer). 

For the optimizer, there are many different choices available,
but to keep things simple, we will start out using an optimizer called \textbf{Adam} for training all of our models. Using a different optimizer can make the 
model fit to the dataset faster or slower. 

\subsubsection{Prediction}
To make the model work for our third goal, where we will predict the actual value for the participant's MADRS score, we have to change a few layers.
To simplify it a bit, we removed two of the \textit{Conv1D} layers, and after the \textit{GlobalAveragePooling1D} layer, this is applied:

\begin{enumerate}
      \setcounter{enumi}{7}
      \item \textbf{Flatten}: The matrix needs to be flat (one dimensional) before proceeding to the final layers.
      \item \textbf{Dense}: A dense layer with $10$ neurons, and \textit{relu} as the activation function. 
      \item \textbf{Dense}: The output layer is a dense layer of size $1$, since we are predicting \textit{one} value. 
            Also, the activation on this layer is going to be \textit{linear} instead of softmax.
\end{enumerate}

We compile this model with the loss function \textit{Mean Squared Error}, which is measured as the average (mean) 
of squared difference between predictions and actual observations \cite{loss_functions}. The optimizer is the same as before.

\subsection{Creating Input and Output Data}
\begin{table}
  \begin{center}
    \begin{tabular}{| l | l |}
      \hline
      \textbf{Not bipolar} & \textbf{Bipolar}  \\ \hline
      0                    &  1                \\ \hline
      1                    &  0                \\ \hline
      0                    &  1                \\ \hline
      1                    &  0                \\ \hline
      1                    &  0                \\ \hline
      0                    &  1                \\ \hline
      0                    &  1                \\ \hline
      1                    &  0                \\ \hline
    \end{tabular}
    \caption{Categorical Labels}
    \label{table:categorical_labels}
  \end{center}
\end{table}

One function (\textit{create\_segments\_and\_labels()} \ref{code:reading_dataset}) is responsible of creating the data that is sent into the neural network. 
We start by defining a \textit{segment length} $L$, which is how much data (minutes) we want inside each segment. We will experiment with the value of $L$, 
but let's say we use segments of 4 hours at a time ($L=4*60=240$). Next we need a value for how many values to step after each iteration, $S$. 
We will keep this value at one hour, meaning $S=60$. Between the different goals, this function will only be different in how it determines the output values.
The code in the Source Code section \ref{code:reading_dataset} is simplified to only generate input and output for 
classifying control / condition groups (goal one).

\begin{itemize}
  \item First we read the \textit{global} dataset, where we find each participant and whether they are bipolar or not. As there is no \textit{afftype} value
        for non-bipolar participants, we simply set this to 0. This is fine because the other possible values are 1, 2 and 3.
  \item Then we iterate over all participant activity data files:
  \begin{itemize}
    \item Append a \textbf{segment} that is of length $L$ to the list of segments (using default parameters in the
          \textit{create\_segments\_and\_labels} function \ref{code:reading_dataset}).
    \item Append the target value for the current goal, so:
      \begin{itemize}
            \item Append a $1$ or $0$ for classifying control/condition group.
            \item Append a $0$, $1$ or $2$ for classifying depression class \\(normal/mild/moderate).
            \item Append the MADRS score (after measurement period) when the goal is to predict MADRS score.
      \end{itemize}  
    \item Skip $S$ indexes, and repeat until we have added all segments.
  \end{itemize}
  \item Make the list of labels into a \textit{categorical} 2D matrix \ref{table:categorical_labels} with a \textbf{1} in only one of the columns,
        instead of a single-dimensional list.
        This is only needed in the first two goals, for the \textbf{softmax} activation function.
  \item Also we need the list of segments to be restructured. We do this with the \textbf{reshape} function, 
        and after this the data is ready to be passed into the neural network.
\end{itemize}

\subsubsection{Train and test data}
One last step is done before we can start training the model. We need to split the data in two parts; training and test data. This way we can calculate 
the performance of the model after training has finished, and also prevent overfitting by evaluating on data that is unseen to the model. 

The function \textit{train\_test\_split} from the \textit{sklearn} package is useful here, where you input the segments and labels, 
plus how large you want the training and test sets to be (number between 0 and 1, which determines the size of the test partition). 
The function also randomizes the data, preventing model to accidentally learn something that is correct for segments in a row that 
also are chronologically in order. After calling the function \ref{code:sklearn_train_test_split}, you end up with two arrays for input data
(\textit{X\_train, X\_test}), and two arrays for output data (\textit{y\_train, y\_test}). In this case the \textit{test\_size} is set to $0.4$, 
meaning that the test sets contains $40\%$ of the total dataset and the remaining $60\%$ are in the training sets.

\begin{code}
    \caption{Sklearn train and test split}
    \label{code:sklearn_train_test_split}
    
    \begin{minted}[linenos]{python}
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(segments, 
                                                    labels, 
                                                    test_size=0.4)
    \end{minted}
\end{code}

\noindent Another option is to use something called \textit{K-fold cross-validation}. It works by splitting the dataset into $K$ train and test sets, 
and for each of them, train the model on the training set and validate on the test set. This is a good way of checking if your model's performance 
truly is independent on which data it trained on. The higher number of splits ($K$) means less data to train on, so you need to keep that in mind 
(also applies to \textit{train\_test\_split} if you set the \textit{test\_size} too high). Sklearn has an implementation of \textit{K-fold} 
splitting of data, called \textit{StratifiedKFold}. For example, code \ref{code:sklearn_k_fold} shows how you can use \textit{StratifiedKFold}
to perform a 3-fold cross-validation in your project ($K = 3$). 

\begin{code}
    \caption{Sklearn K-Fold}
    \label{code:sklearn_k_fold}
    
    \begin{minted}[linenos]{python}
    from sklearn.model_selection import StratifiedKFold

    segments, labels = create_segments_and_labels(...)

    skf = StratifiedKFold(n_splits=3, shuffle=True)

    for train_index, test_index in skf.split(segments, labels):
        X_train, X_test = segments[train_index], segments[test_index]
        y_train, y_test = labels[train_index], labels[test_index]

        model = create_model(...)
        model.fit(X_train, y_train, ...)
        results = model.evaluate(X_test, y_test)

        # do something with results...

    \end{minted}
\end{code}

\section{Optimizing the model}

\noindent Out of the box, we don't think that the model is going to perform perfectly. Therefore we need to experiment with each parameter that we pass to
the model, and find the best ones. We have some ideas of what to change:

\begin{itemize}
    \item Use different segment lengths
    \item Tweak hyper-parameters
\end{itemize}

\noindent The first idea, using different segment lengths, is the one we think is going to impact the results the most. Having more data inside each segment 
will give the neural network more opportunities to learn features, and then be better at its job of either classifying or predicting something. 
It will however take more time to train the model, since each layer has to go through more data. Is it worth waiting the training time 
for increased segment lengths through, and is there one segment length that is optimal to use for shortest training time and best results?
This question will be answered later, when we have gathered the results.

Hyper-parameters are all the \textit{higher-order} parameters we compile/train the model with. They are different from the parameters learned by
training the model, and need to be fixed for one session of model training. Finding the perfect ones can be crucial for a well performing model. 
Hyper-parameters that we use in our models include:

\begin{itemize}
    \item \textbf{Optimizer and learning rate}\\
        The \textbf{optimizer} function (as said earlier) is important to lower the loss value when fitting the dataset to the model. 
        Different optimizers have different input parameters, and the \textit{learning rate} is one that all optimizers use. 
        Tweaking the learning rate can yield better results, 
        but it's generally a good idea to start with default learning rate. % find source?
    \item \textbf{Epochs}\\
        Defines how many iterations of training that are to be executed, and in most cases more epochs yield better results, 
        up to a certain point.
    \item \textbf{Batch size}\\
        This is how much data that is processed at the same time each epoch, and the best batch size to use can be completely different on 
        two different models.  
    \item \textbf{Train/test split size}\\
        When building a neural network, we want as much data as possible to both train and to test on afterwards. We can't use all the data 
        in both cases, so the most balanced split size needs to be determined.
\end{itemize}

\newpage

\section{Source Code}
\subsection{Create Input Data}
\input{code/tex/create_segments_and_labels}

\newpage
\subsection{The Models}
\input{code/tex/1d_conv_net}

% maybe move to other chapter?

\section{Training the models}

For training large neural networks you often need expensive hardware if you want to do it on your own. 
The way to do it these days is to use cloud services such as Google or Amazon to run the machine learning once you have prepared the dataset and the model. 
These services are often paid by the hour, but it's not expensive as long as you remember to shut it down after use, and only run it when you really need to.

Our dataset, however, only contains 55 participants and only a few days of activity measurements for each participant, 
so using a service to train the model would not be necessary. We decided to train them on one of our own personal computers, 
with an Nvidia GTX 1070. Our guess is that this GPU will do the job just fine. 
 

\subsection{Control vs Condition Group}
\subsubsection{Results}

\subsection{Depression Classes}
\subsubsection{Results}

\subsection{MADRS Score}
\subsubsection{Results}