\section{Regression}

\section{1D Convolutional Neural Network}

\subsection{Creating the Model}
Following the tutorial on 1D CNNs \cite{1d_cnn}, we came up with this model \ref{code:1d_conv_net}. 
We used most of the same parameters, but we also had to tweak some of them to make the model work on our dataset.
To make a convolutional neural network, you always need some \textit{convolutional} and \textit{pooling} layers.
Which layers you add in between and the ordering of them, together with the parameters you pass to the layers, 
is what makes the model perform differently. This, of course, requires your input data to be well structured.

\begin{itemize}
  \item We start by defining a \textbf{Sequential} model. This is an easy to understand and readable way of defining a model. 
        Alternatively we could use a \textbf{functional} model, which gives more control of inputs and outputs of the layers.
        A \textbf{functional} model would be useful if we wanted to debug and optimize each layer within the model.
  \item \textbf{Reshape}: In the first layer we need to reshape the input data so that it becomes an $X$ by 1 matrix, where $X$ is the length of each segment.
        The reason for this reshape step is because the next layer (\textbf{Conv1D}) requires the input to contain (\textit{batch, steps, channels}). 
        \textit{Batch} will be set to \textbf{None}, \textit{steps} will be the segments, and \textit{channels} will be $1$ 
        (because we only have one measurement value for each minute).
  \item \textbf{Conv1D}: This is the first \textit{convolutional} layer, where the required parameters are how many \textit{filters} 
        you want, and how big the \textit{kernel} should be. As in the tutorial, we also use 100 filters and a kernel size of 10. 
        We don't know if these are the perfect values to use, but having 100 filters means that the network can learn 100 features in this layer.
        Having less or more might impact the performance, which we will experiment with in the section about model optimization.
        There are many different parameters that you can use on a layer like this, for example \textit{padding} and \textit{strides},
        but using the default values is good for now. This is also something that we can experiment with later.
        However using the default parameters, the output of this layer will be a $X-9 \times 100$ matrix, where $X$ is the same as in the first layer.
  \item \textbf{Conv1D}: The second convolutional layer will look exactly like the first one, and the output will be a $X-9-9 \times 100$ matrix. 
  \item \textbf{MaxPooling1D}: Pooling is important in a convolutional neural network to reduce complexity \cite{1d_cnn}. 
        The basic idea of \textit{max pooling} is to reduce to only the maximum value for each \textit{window} of size $N \times N$. We are using 2 as 
        window size ($N$), resulting in matrix that is half the size of the input ($ \frac{X-9-9}{2} \times 100$). 
        Pooling may also help reduce \textit{overfitting}.
  \item \textbf{Conv1D}: Two more convolutional layers are added, and after these the input to the next layer will be a matrix of size
        $ (\frac{X-9-9}{2}-9-9) \times 160 $.
  \item \textbf{GlobalAveragePooling1D}: This is another pooling layer, and this one takes the average of weights within the network instead of the maximum.
        After doing this, the matrix will be of size $ 1 \times 160 $.
  %\item \textbf{Dropout}: Dropout
  \item \textbf{Dense}: The final layer in the model is a dense layer (fully connected) which reduces the matrix from $160$ values to $2$ with the
        activation function \textbf{softmax}. Then output (a $1$ in either of the two neurons), is mapped to the corresponding label (bipolar/not).
        
\end{itemize}

\subsection{Creating Input Data}
\begin{table}
  \begin{center}
    \begin{tabular}{| l | l |}
      \hline
      \textbf{Not bipolar} & \textbf{Bipolar}  \\ \hline
      0                    &  1                \\ \hline
      1                    &  0                \\ \hline
      0                    &  1                \\ \hline
      1                    &  0                \\ \hline
      1                    &  0                \\ \hline
      0                    &  1                \\ \hline
      0                    &  1                \\ \hline
      1                    &  0                \\ \hline
    \end{tabular}
    \caption{Categorical Labels}
    \label{table:categorical_labels}
  \end{center}
\end{table}

One function (\textit{create\_segments\_and\_labels()} \ref{code:reading_dataset}) is responsible of creating the data that is sent into the neural network. 
It does the following:

\begin{itemize}
  \item First we read the \textit{global} dataset, where we find each participant and whether they are bipolar or not. As there is no \textit{afftype} value
        for non-bipolars participants, we simply set this to 0. This is fine because the other possible values are 1, 2 and 3.
  \item Then we iterate over all participant activity data files:
  \begin{itemize}
    \item Append a \textbf{segment} that is $ 8*60=480 $ minutes long to the list of segments.
    \item Append a 1 if the participant is bipolar, or a 0 if not to the list of labels.
    \item Skip to the next hour, and repeat until we have added all segments.
  \end{itemize}
  \item Make the list of labels into a \textit{categorical} 2D matrix \ref{table:categorical_labels} with a \textbf{1} in only one of the columns,
        instead of a single-dimentional list.
        This is needed for the \textbf{softmax} activation function, which we will describe later.
  \item Also we need the list of segments to be restructured. We do this with the \textbf{reshape} function, 
        and after this the data is ready to be passed into the neural network.
\end{itemize}

However, one last step is done before we start training the model. We need to split the data into training and test data, so that we can test how
good the model is after training it. The \textbf{train\_test\_split} from the \textit{sklearn} package does this, and you input the segments and labels,
and also specify how large  you want the training and test sets to be. The function also randomizes the data, so that the model won't accidentally 
learn something that is correct for two segments in a row that also happen to be chronological.

\section{Optimizing the model}

\newpage

\section{Source Code}
\subsection{Create Input Data}
\input{code/tex/create_segments_and_labels}

\subsection{The Model}
\input{code/tex/1d_conv_net}